{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOx80br07yJi"
   },
   "source": [
    "Kütüphanelerin Kurulumu ve Google Drive Bağlantısı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QU-_T5dS7b25",
    "outputId": "86683ab4-c8f7-4a48-8279-af6208e1ed15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Mounted at /content/drive\n",
      "Kullanılan cihaz: cuda\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch torchvision Pillow accelerate -U\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTFeatureExtractor, ViTModel, BertTokenizer, BertModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Kullanılan cihaz: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30vqxV2h768S"
   },
   "source": [
    "Sabitler ve Veri Yolları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkhCWi2972Or"
   },
   "outputs": [],
   "source": [
    "DRIVE_BASE_PATH = '/content/drive/MyDrive/DerinOgrenme/'\n",
    "\n",
    "IMAGES_ZIP_PATH = os.path.join(DRIVE_BASE_PATH, 'images.zip')\n",
    "CAPTIONS_JSON_PATH = os.path.join(DRIVE_BASE_PATH, 'captions.json')\n",
    "LABELS_ZIP_PATH = os.path.join(DRIVE_BASE_PATH, 'labels.zip')\n",
    "KEYPOINTS_ZIP_PATH = os.path.join(DRIVE_BASE_PATH, 'keypoints.zip')\n",
    "SEGM_ZIP_PATH = os.path.join(DRIVE_BASE_PATH, 'segm.zip')\n",
    "DENSEPOSE_ZIP_PATH = os.path.join(DRIVE_BASE_PATH, 'densepose.zip')\n",
    "\n",
    "COLAB_WORKING_DIR = '/content/DeepFashionMultiModal_data/'\n",
    "IMAGE_DIR_UNZIPPED = os.path.join(COLAB_WORKING_DIR, 'images')\n",
    "\n",
    "os.makedirs(COLAB_WORKING_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR_UNZIPPED, exist_ok=True)\n",
    "\n",
    "IMG_MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n",
    "TEXT_MODEL_NAME = 'bert-base-uncased'\n",
    "EMBEDDING_DIM = 512\n",
    "TEMPERATURE = 0.07\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwFWsouC9sYg",
    "outputId": "3997ba73-2b86-4dcc-8569-f97c0e523260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Görsel dosyaları işleniyor...\n",
      "'images.zip' dosyası '/content/DeepFashionMultiModal_data/images' klasörüne çıkarılıyor...\n",
      "Çıkarma işlemi tamamlandı: '/content/DeepFashionMultiModal_data/images'\n",
      "Görseller başarıyla '/content/DeepFashionMultiModal_data/images' klasörüne çıkarıldı.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_path, extract_to_path):\n",
    "    \"\"\"Belirtilen zip dosyasını hedef yola çıkarır.\"\"\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"HATA: Zip dosyası bulunamadı: {zip_path}\")\n",
    "        return False\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            print(f\"'{os.path.basename(zip_path)}' dosyası '{extract_to_path}' klasörüne çıkarılıyor...\")\n",
    "            zip_ref.extractall(extract_to_path)\n",
    "        print(f\"Çıkarma işlemi tamamlandı: '{extract_to_path}'\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Zip dosyası çıkarılırken hata oluştu: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"Görsel dosyaları işleniyor...\")\n",
    "\n",
    "if not os.listdir(IMAGE_DIR_UNZIPPED):\n",
    "    if unzip_file(IMAGES_ZIP_PATH, IMAGE_DIR_UNZIPPED):\n",
    "        print(f\"Görseller başarıyla '{IMAGE_DIR_UNZIPPED}' klasörüne çıkarıldı.\")\n",
    "    else:\n",
    "        print(f\"Görseller çıkarılamadı. Lütfen '{IMAGES_ZIP_PATH}' dosyasını kontrol edin.\")\n",
    "else:\n",
    "    print(f\"'{IMAGE_DIR_UNZIPPED}' klasörü zaten dolu. Zip çıkarma işlemi atlandı.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FnMETkwHtJI"
   },
   "outputs": [],
   "source": [
    "IMAGE_DIR_UNZIPPED = '/content/DeepFashionMultiModal_data/images/images'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17tgOr5M8CXf"
   },
   "source": [
    "Veri Yükleme ve Özel Dataset Sınıfı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "2100f28dae034f8bbd6bcb821eabf22e",
      "ef4c21faea2241f394db654fd7875d6e",
      "885e23a8fe6e479486332b7ed6993f3c",
      "b8744e25515b42e69af30657ea35f7b3",
      "54ffc9d0695b4629a673a2be3d8d524e"
     ]
    },
    "id": "be1BC4gB8DQ7",
    "outputId": "e1eea782-3f4a-4c26-bd6b-200d60573e6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2100f28dae034f8bbd6bcb821eabf22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4c21faea2241f394db654fd7875d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885e23a8fe6e479486332b7ed6993f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8744e25515b42e69af30657ea35f7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ffc9d0695b4629a673a2be3d8d524e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metin açıklamaları (captions.json) yükleniyor...\n",
      "JSON formatı: Sözlük (image_filename -> caption). Doğrudan kullanılıyor.\n",
      "42544 adet geçerli görsel-metin çifti bulundu.\n",
      "Veri Seti Boyutları: Toplam=42544, Eğitim=29780, Doğrulama=6381, Test=6383\n",
      "Veri yükleyiciler (eğitim, doğrulama, test) hazır.\n"
     ]
    }
   ],
   "source": [
    "def load_text_descriptions(json_file_path):\n",
    "    if not os.path.exists(json_file_path):\n",
    "        print(f\"HATA: JSON dosyası bulunamadı: {json_file_path}\")\n",
    "        return None\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        descriptions = json.load(f)\n",
    "    if isinstance(descriptions, list) and descriptions and isinstance(descriptions[0], dict) and \"image\" in descriptions[0] and \"caption\" in descriptions[0]:\n",
    "        print(\"JSON formatı: Liste içinde sözlükler. Dönüştürülüyor...\")\n",
    "        return {item[\"image\"]: item[\"caption\"] for item in descriptions}\n",
    "    elif isinstance(descriptions, dict):\n",
    "        print(\"JSON formatı: Sözlük (image_filename -> caption). Doğrudan kullanılıyor.\")\n",
    "        return descriptions\n",
    "    else:\n",
    "        print(\"HATA: captions.json dosyasının formatı anlaşılamadı. Lütfen yapıyı kontrol edin.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "class FashionMultiModalDataset(Dataset):\n",
    "    def __init__(self, image_root_dir, text_descriptions_dict, img_processor, text_tokenizer, max_text_len=128):\n",
    "        self.image_root_dir = image_root_dir\n",
    "        self.text_descriptions_dict = text_descriptions_dict\n",
    "\n",
    "        self.valid_items = []\n",
    "        available_images = set(os.listdir(self.image_root_dir))\n",
    "\n",
    "        for img_filename, description in self.text_descriptions_dict.items():\n",
    "            if img_filename in available_images:\n",
    "                self.valid_items.append({\"image_filename\": img_filename, \"description\": description})\n",
    "\n",
    "        if not self.valid_items:\n",
    "            raise ValueError(\"Açıklamalar ve görseller eşleştirildikten sonra geçerli öğe bulunamadı.\")\n",
    "\n",
    "        print(f\"{len(self.valid_items)} adet geçerli görsel-metin çifti bulundu.\")\n",
    "\n",
    "        self.img_processor = img_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.valid_items[idx]\n",
    "        img_filename = item[\"image_filename\"]\n",
    "        text_desc = item[\"description\"]\n",
    "\n",
    "        img_path = os.path.join(self.image_root_dir, img_filename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image_processed = self.img_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Görsel yüklenirken hata: {img_path}, Hata: {e}\")\n",
    "            return None\n",
    "\n",
    "        text_tokenized = self.text_tokenizer(\n",
    "            text_desc,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = text_tokenized['input_ids'].squeeze(0)\n",
    "        attention_mask = text_tokenized['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'pixel_values': image_processed,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "img_processor = ViTFeatureExtractor.from_pretrained(IMG_MODEL_NAME)\n",
    "text_tokenizer = BertTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "\n",
    "print(\"Metin açıklamaları (captions.json) yükleniyor...\")\n",
    "all_text_descriptions = load_text_descriptions(CAPTIONS_JSON_PATH)\n",
    "\n",
    "if all_text_descriptions:\n",
    "    try:\n",
    "        dataset_full = FashionMultiModalDataset(IMAGE_DIR_UNZIPPED, all_text_descriptions, img_processor, text_tokenizer)\n",
    "\n",
    "        if len(dataset_full) >= 3:\n",
    "            train_size = int(0.7 * len(dataset_full))\n",
    "            val_size = int(0.15 * len(dataset_full))\n",
    "            test_size = len(dataset_full) - train_size - val_size\n",
    "\n",
    "            if test_size <= 0 and val_size > 1:\n",
    "                test_size = max(1, int(0.05 * len(dataset_full)))\n",
    "                val_size = len(dataset_full) - train_size - test_size\n",
    "                if val_size <=0:\n",
    "                    val_size = 0\n",
    "                    test_size = len(dataset_full) - train_size\n",
    "            elif test_size <= 0:\n",
    "                 raise ValueError(\"Test seti için yeterli veri ayrılamadı. Veri setiniz çok küçük olabilir.\")\n",
    "\n",
    "\n",
    "            print(f\"Veri Seti Boyutları: Toplam={len(dataset_full)}, Eğitim={train_size}, Doğrulama={val_size}, Test={test_size}\")\n",
    "\n",
    "            if train_size <=0 or (val_size < 0) or (test_size <0) or (train_size + val_size + test_size != len(dataset_full)):\n",
    "                print(\"HATA: Veri seti bölme boyutları geçersiz. Lütfen kontrol edin.\")\n",
    "                print(f\"train_size: {train_size}, val_size: {val_size}, test_size: {test_size}, toplam: {len(dataset_full)}\")\n",
    "                raise SystemExit(\"Veri seti bölme hatası.\")\n",
    "\n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "                dataset_full, [train_size, val_size, test_size]\n",
    "            )\n",
    "\n",
    "            def collate_fn(batch):\n",
    "                batch = list(filter(lambda x: x is not None, batch))\n",
    "                if not batch: return None\n",
    "                return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "            if val_size > 0:\n",
    "                val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "            else:\n",
    "                val_dataloader = None\n",
    "                print(\"Uyarı: Doğrulama seti oluşturulmadı (val_size=0).\")\n",
    "\n",
    "            if test_size > 0:\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "            else:\n",
    "                test_dataloader = None\n",
    "                print(\"Uyarı: Test seti oluşturulmadı (test_size=0).\")\n",
    "\n",
    "            print(\"Veri yükleyiciler (eğitim, doğrulama, test) hazır.\")\n",
    "        else:\n",
    "            print(\"HATA: Dataset oluşturuldu ancak geçerli öğe bulunamadı veya test için yeterli değil.\")\n",
    "    except:\n",
    "        print(\"HATA: Dataset oluşturulurken hata oluştu.\")\n",
    "else:\n",
    "    print(\"HATA: Metin açıklamaları yüklenirken hata oluştu.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qiq8B45z8JCK"
   },
   "source": [
    "Çok Modlu Modelin Tanımlanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185,
     "referenced_widgets": [
      "87c2add5dcea49fc82987c98ceaceeef",
      "c1a1638b4ef34f40a15b050ee59dc353",
      "95d8ace806cd43bb96a9e47d6a47b16f"
     ]
    },
    "id": "7EUajVaa8Kv4",
    "outputId": "235fd19b-0129-4bac-ae5a-b474339b0bde"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c2add5dcea49fc82987c98ceaceeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a1638b4ef34f40a15b050ee59dc353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d8ace806cd43bb96a9e47d6a47b16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model başarıyla oluşturuldu ve cihaza taşındı.\n"
     ]
    }
   ],
   "source": [
    "class MultiModalFashionModel(nn.Module):\n",
    "    def __init__(self, img_model_name, text_model_name, embedding_dim, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ViTModel.from_pretrained(img_model_name)\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model_name)\n",
    "\n",
    "        img_feature_dim = self.image_encoder.config.hidden_size\n",
    "        text_feature_dim = self.text_encoder.config.hidden_size\n",
    "\n",
    "        self.image_projection = nn.Linear(img_feature_dim, embedding_dim)\n",
    "        self.text_projection = nn.Linear(text_feature_dim, embedding_dim)\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / temperature))\n",
    "\n",
    "\n",
    "    def encode_image(self, pixel_values):\n",
    "        outputs = self.image_encoder(pixel_values=pixel_values)\n",
    "        image_features = outputs.pooler_output\n",
    "        image_embedding = self.image_projection(image_features)\n",
    "        return F.normalize(image_embedding, p=2, dim=-1)\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = outputs.pooler_output\n",
    "        text_embedding = self.text_projection(text_features)\n",
    "        return F.normalize(text_embedding, p=2, dim=-1)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        image_embeddings = self.encode_image(pixel_values)\n",
    "        text_embeddings = self.encode_text(input_ids, attention_mask)\n",
    "        return image_embeddings, text_embeddings\n",
    "\n",
    "    def contrastive_loss(self, image_embeddings, text_embeddings):\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_embeddings @ text_embeddings.t()\n",
    "        logits_per_text = logit_scale * text_embeddings @ image_embeddings.t()\n",
    "\n",
    "        batch_size = image_embeddings.shape[0]\n",
    "        labels = torch.arange(batch_size, device=image_embeddings.device)\n",
    "\n",
    "        loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_text = F.cross_entropy(logits_per_text, labels)\n",
    "\n",
    "        loss = (loss_img + loss_text) / 2.0\n",
    "        return loss\n",
    "\n",
    "model = MultiModalFashionModel(IMG_MODEL_NAME, TEXT_MODEL_NAME, EMBEDDING_DIM, TEMPERATURE).to(device)\n",
    "print(\"Model başarıyla oluşturuldu ve cihaza taşındı.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UA_nwKm8NGt"
   },
   "source": [
    "Eğitim Döngüsü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 361,
     "referenced_widgets": [
      "3fd3fce0461448ccae88b013560d4920",
      "88c06b48031243aca10a1e628914e37c",
      "b0d7ea34c91e465596b9001a00c92246",
      "82b56e21d2a242c086e2014702c481ee",
      "22a065b12fab42adba5e4197e73f0ee8",
      "6f27649276a146d2be214ab0ab5a859c",
      "e4e23f22d86e4e6694eeeb8186c98df2"
     ]
    },
    "id": "uhZlU5y98P1B",
    "outputId": "0231eed2-6e14-4427-a2e2-39cbbe209706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim başlıyor...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd3fce0461448ccae88b013560d4920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eğitim Adımı:   0%|          | 0/1862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama Eğitim Kaybı: 0.9512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c06b48031243aca10a1e628914e37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doğrulama Adımı:   0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama Doğrulama Kaybı: 0.4757\n",
      "En iyi model kaydedildi: best_multimodal_fashion_model.pth\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d7ea34c91e465596b9001a00c92246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eğitim Adımı:   0%|          | 0/1862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama Eğitim Kaybı: 0.3563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b56e21d2a242c086e2014702c481ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doğrulama Adımı:   0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama Doğrulama Kaybı: 0.3193\n",
      "En iyi model kaydedildi: best_multimodal_fashion_model.pth\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a065b12fab42adba5e4197e73f0ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eğitim Adımı:   0%|          | 0/1862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama Eğitim Kaybı: 0.2345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f27649276a146d2be214ab0ab5a859c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Doğrulama Adımı:   0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama Doğrulama Kaybı: 0.2582\n",
      "En iyi model kaydedildi: best_multimodal_fashion_model.pth\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e23f22d86e4e6694eeeb8186c98df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eğitim Adımı:   0%|          | 0/1862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Eğitim Adımı\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        if batch is None: continue\n",
    "\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_embeddings, text_embeddings = model(pixel_values, input_ids, attention_mask)\n",
    "        loss = model.contrastive_loss(image_embeddings, text_embeddings)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, device):\n",
    "    if dataloader is None:\n",
    "        print(\"Doğrulama dataloader'ı yok, doğrulama adımı atlanıyor.\")\n",
    "        return float('inf')\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Doğrulama Adımı\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            if batch is None: continue\n",
    "\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            image_embeddings, text_embeddings = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = model.contrastive_loss(image_embeddings, text_embeddings)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'val_loss': loss.item()})\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "print(\"Eğitim başlıyor...\")\n",
    "best_val_loss = float('inf')\n",
    "if 'train_dataloader' not in locals() or train_dataloader is None:\n",
    "    print(\"HATA: Eğitim veri yükleyici (train_dataloader) tanımlanmamış veya boş. Lütfen Hücre 3'ü kontrol edin.\")\n",
    "else:\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "        print(f\"Ortalama Eğitim Kaybı: {train_loss:.4f}\")\n",
    "\n",
    "        if 'val_dataloader' in locals() and val_dataloader is not None:\n",
    "            val_loss = validate_epoch(model, val_dataloader, device)\n",
    "            print(f\"Ortalama Doğrulama Kaybı: {val_loss:.4f}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), 'best_multimodal_fashion_model.pth')\n",
    "                print(\"En iyi model kaydedildi: best_multimodal_fashion_model.pth\")\n",
    "        else:\n",
    "            print(\"Doğrulama yapılmıyor. Model her epoch sonunda kaydedilmiyor (sadece en iyi val_loss'a göre).\")\n",
    "\n",
    "\n",
    "print(\"Eğitim tamamlandı.\")\n",
    "if 'val_dataloader' not in locals() or val_dataloader is None:\n",
    "     torch.save(model.state_dict(), 'final_multimodal_fashion_model_no_val.pth')\n",
    "     print(\"Son model (doğrulama yapılmadı) kaydedildi: final_multimodal_fashion_model_no_val.pth\")\n",
    "     model_to_evaluate_path = 'final_multimodal_fashion_model_no_val.pth'\n",
    "elif os.path.exists('best_multimodal_fashion_model.pth'):\n",
    "    model_to_evaluate_path = 'best_multimodal_fashion_model.pth'\n",
    "    print(\"En iyi doğrulanmış model test için kullanılacak.\")\n",
    "else:\n",
    "    print(\"Kaydedilmiş bir model bulunamadı. Son epoch'taki model test edilecek (eğer varsa).\")\n",
    "    model_to_evaluate_path = None\n",
    "\n",
    "if 'test_dataloader' in locals() and test_dataloader is not None:\n",
    "    print(\"\\nTest seti üzerinde son değerlendirme yapılıyor...\")\n",
    "    if model_to_evaluate_path and os.path.exists(model_to_evaluate_path):\n",
    "        print(f\"Test için model yükleniyor: {model_to_evaluate_path}\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_to_evaluate_path, map_location=device))\n",
    "            print(\"Model başarıyla yüklendi.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Model yüklenirken hata: {e}. Mevcut model durumuyla devam edilecek (eğer varsa).\")\n",
    "    elif 'model' not in locals() or model is None :\n",
    "         print(\"Test için yüklenecek bir model veya eğitilmiş bir model nesnesi bulunamadı.\")\n",
    "    else:\n",
    "        print(\"Kaydedilmiş model bulunamadı. Eğitimden sonraki son model durumuyla test ediliyor.\")\n",
    "\n",
    "    if 'model' in locals() and model is not None:\n",
    "        final_scores = evaluate_model(model, test_dataloader, device)\n",
    "        if final_scores:\n",
    "            print(\"\\nDeğerlendirme tamamlandı.\")\n",
    "    else:\n",
    "        print(\"Değerlendirme yapılamadı: Model nesnesi mevcut değil.\")\n",
    "else:\n",
    "    print(\"Test veri yükleyici (test_dataloader) bulunamadığından son değerlendirme yapılamadı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RSXT9lR_B88"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "\n",
    "    print(\"Test seti için tüm embedding'ler çıkarılıyor...\")\n",
    "    for batch in tqdm(dataloader, desc=\"Embedding Çıkarma (Test)\"):\n",
    "        if batch is None: continue\n",
    "\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        image_embeddings, text_embeddings = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "        all_image_embeddings.append(image_embeddings.cpu())\n",
    "        all_text_embeddings.append(text_embeddings.cpu())\n",
    "\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "        print(\"Hata: Embedding listeleri boş. Dataloader'da sorun olabilir.\")\n",
    "        return None, None\n",
    "\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "\n",
    "    return all_image_embeddings, all_text_embeddings\n",
    "\n",
    "def calculate_retrieval_metrics(image_embeddings, text_embeddings, k_values=[1, 5, 10]):\n",
    "    num_samples = image_embeddings.shape[0]\n",
    "    if num_samples == 0:\n",
    "        return {f\"R@{k} (I2T)\": 0.0 for k in k_values}, {f\"R@{k} (T2I)\": 0.0 for k in k_values}\n",
    "\n",
    "    sim_i2t = image_embeddings @ text_embeddings.t()\n",
    "\n",
    "    i2t_metrics = {}\n",
    "    for k in k_values:\n",
    "        correct_at_k = 0\n",
    "        for i in range(num_samples):\n",
    "            query_sim = sim_i2t[i]\n",
    "            _, top_k_indices = torch.topk(query_sim, k=k, largest=True)\n",
    "            if i in top_k_indices:\n",
    "                correct_at_k += 1\n",
    "        i2t_metrics[f\"R@{k} (I2T)\"] = correct_at_k / num_samples\n",
    "\n",
    "    sim_t2i = text_embeddings @ image_embeddings.t()\n",
    "\n",
    "    t2i_metrics = {}\n",
    "    for k in k_values:\n",
    "        correct_at_k = 0\n",
    "        for i in range(num_samples):\n",
    "            query_sim = sim_t2i[i]\n",
    "            _, top_k_indices = torch.topk(query_sim, k=k, largest=True)\n",
    "            if i in top_k_indices:\n",
    "                correct_at_k += 1\n",
    "        t2i_metrics[f\"R@{k} (T2I)\"] = correct_at_k / num_samples\n",
    "\n",
    "    return i2t_metrics, t2i_metrics\n",
    "\n",
    "def evaluate_model(model, test_dataloader, device, k_values=[1, 5, 10]):\n",
    "    if test_dataloader is None:\n",
    "        print(\"Test dataloader'ı yok. Değerlendirme atlanıyor.\")\n",
    "        return None\n",
    "\n",
    "    image_embeddings, text_embeddings = get_all_embeddings(model, test_dataloader, device)\n",
    "\n",
    "    if image_embeddings is None or text_embeddings is None or image_embeddings.shape[0] == 0:\n",
    "        print(\"Değerlendirme için yeterli embedding çıkarılamadı.\")\n",
    "        return None\n",
    "\n",
    "    if image_embeddings.shape[0] != text_embeddings.shape[0]:\n",
    "        print(f\"Hata: Görüntü ({image_embeddings.shape[0]}) ve metin ({text_embeddings.shape[0]}) embedding sayıları eşleşmiyor.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n{image_embeddings.shape[0]} örnek üzerinde geri getirme metrikleri hesaplanıyor...\")\n",
    "    i2t_scores, t2i_scores = calculate_retrieval_metrics(image_embeddings, text_embeddings, k_values)\n",
    "\n",
    "    print(\"\\n--- Görüntüden Metne Geri Getirme (I2T) Skorları ---\")\n",
    "    for metric, score in i2t_scores.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Metinden Görüntüye Geri Getirme (T2I) Skorları ---\")\n",
    "    for metric, score in t2i_scores.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "    return {\"i2t\": i2t_scores, \"t2i\": t2i_scores}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thTorXq58T5M"
   },
   "source": [
    "Modelin Yüklenmesi ve Embedding Çıkarma (Inference Örneği)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJcGaKlT8Wp4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_image_embedding(model, image_path, img_processor, device):\n",
    "    model.eval()\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_processed = img_processor(images=image, return_tensors=\"pt\")['pixel_values'].to(device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = model.encode_image(image_processed)\n",
    "        return image_embedding.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Görsel embedding'i çıkarılırken hata: {image_path}, Hata: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_text_embedding(model, text_description, text_tokenizer, device, max_text_len=128):\n",
    "    model.eval()\n",
    "    text_tokenized = text_tokenizer(\n",
    "        text_description,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_text_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = text_tokenized['input_ids'].to(device)\n",
    "    attention_mask = text_tokenized['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embedding = model.encode_text(input_ids, attention_mask)\n",
    "    return text_embedding.cpu().numpy()\n",
    "\n",
    "if 'val_dataset' in locals() and len(val_dataset) > 0 :\n",
    "    try:\n",
    "        sample_idx = np.random.randint(0, len(val_dataset.dataset.image_filenames))\n",
    "        sample_img_filename = val_dataset.dataset.image_filenames[sample_idx]\n",
    "        sample_img_path = os.path.join(IMAGE_DIR, sample_img_filename)\n",
    "        sample_text_desc = val_dataset.dataset.text_descriptions.get(sample_img_filename, \"Açıklama bulunamadı.\")\n",
    "\n",
    "        print(f\"\\nÖrnek Görsel: {sample_img_path}\")\n",
    "        print(f\"Örnek Metin: {sample_text_desc}\")\n",
    "\n",
    "        img_emb = get_image_embedding(model, sample_img_path, img_processor, device)\n",
    "        text_emb = get_text_embedding(model, sample_text_desc, text_tokenizer, device)\n",
    "\n",
    "        if img_emb is not None and text_emb is not None:\n",
    "            print(\"Görsel Embedding Boyutu:\", img_emb.shape)\n",
    "            print(\"Metin Embedding Boyutu:\", text_emb.shape)\n",
    "\n",
    "            similarity = (img_emb @ text_emb.T).item() / (np.linalg.norm(img_emb) * np.linalg.norm(text_emb))\n",
    "            print(f\"Örnek görsel ve metin arası cosine benzerliği: {similarity:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Inference örneği çalıştırılırken hata: {e}\")\n",
    "else:\n",
    "    print(\"Inference örneği için val_dataset bulunamadı veya boş.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c877cy0h8ZEK"
   },
   "source": [
    "Basit Tavsiye Sistemi (Kavramsal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjw4VgbI8akt"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_image_embedding(model, image_path, img_processor, device):\n",
    "    model.eval()\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_processed = img_processor(images=image, return_tensors=\"pt\")['pixel_values'].to(device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = model.encode_image(image_processed)\n",
    "        return image_embedding.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Görsel embedding'i çıkarılırken hata: {image_path}, Hata: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_text_embedding(model, text_description, text_tokenizer, device, max_text_len=128):\n",
    "    model.eval()\n",
    "    text_tokenized = text_tokenizer(\n",
    "        text_description,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_text_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = text_tokenized['input_ids'].to(device)\n",
    "    attention_mask = text_tokenized['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embedding = model.encode_text(input_ids, attention_mask)\n",
    "    return text_embedding.cpu().numpy()\n",
    "\n",
    "if 'dataset' in locals() and len(dataset.valid_items) > 0 :\n",
    "    try:\n",
    "        sample_item_info = dataset.valid_items[np.random.randint(0, len(dataset.valid_items))]\n",
    "        sample_img_filename = sample_item_info[\"image_filename\"]\n",
    "        sample_text_desc = sample_item_info[\"description\"]\n",
    "        sample_img_path = os.path.join(IMAGE_DIR_UNZIPPED, sample_img_filename)\n",
    "\n",
    "        print(f\"\\nÖrnek Görsel: {sample_img_path}\")\n",
    "        print(f\"Örnek Metin: {sample_text_desc}\")\n",
    "\n",
    "        img_emb = get_image_embedding(model, sample_img_path, img_processor, device)\n",
    "        text_emb = get_text_embedding(model, sample_text_desc, text_tokenizer, device)\n",
    "\n",
    "        if img_emb is not None and text_emb is not None:\n",
    "            print(\"Görsel Embedding Boyutu:\", img_emb.shape)\n",
    "            print(\"Metin Embedding Boyutu:\", text_emb.shape)\n",
    "\n",
    "            similarity = (img_emb @ text_emb.T).item() / (np.linalg.norm(img_emb) * np.linalg.norm(text_emb))\n",
    "            print(f\"Örnek görsel ve metin arası cosine benzerliği: {similarity:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Inference örneği çalıştırılırken hata: {e}\")\n",
    "else:\n",
    "    print(\"Inference örneği için 'dataset' bulunamadı veya geçerli öğe içermiyor.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7poAezTAIQG"
   },
   "source": [
    "Makale Kıyaslama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c08xYY5NAH3s"
   },
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_classification_tasks(model, dataloader, device, num_categories, num_attributes, top_k_category=[1, 3, 5]):\n",
    "    model.eval()\n",
    "\n",
    "    total_category_correct = {k: 0 for k in top_k_category}\n",
    "    total_category_samples = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if not hasattr(model, 'predict_category') or not hasattr(model, 'predict_attributes'):\n",
    "        print(\"HATA: Modelde 'predict_category' veya 'predict_attributes' metotları bulunmuyor.\")\n",
    "        print(\"Lütfen modelinizi bu tahmin görevleri için güncelleyin.\")\n",
    "        return None, None\n",
    "\n",
    "    all_category_preds = []\n",
    "    all_category_labels = []\n",
    "    all_attribute_preds_probs = []\n",
    "    all_attribute_labels = []\n",
    "\n",
    "    print(\"Kategori ve Öznitelik tahmini için test seti değerlendiriliyor...\")\n",
    "    for batch in tqdm(dataloader, desc=\"Kategori/Öznitelik Değerlendirme\"):\n",
    "        if batch is None: continue\n",
    "\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        if 'category_label' not in batch or 'attribute_labels' not in batch:\n",
    "            print(\"HATA: Batch'te 'category_label' veya 'attribute_labels' bulunmuyor.\")\n",
    "            continue\n",
    "\n",
    "        category_labels_batch = batch['category_label'].to(device)\n",
    "        attribute_labels_batch = batch['attribute_labels'].to(device, dtype=torch.float)\n",
    "\n",
    "        category_logits = model.predict_category(pixel_values)\n",
    "        _, top_k_preds_category = torch.topk(category_logits, max(top_k_category), dim=1)\n",
    "\n",
    "        total_category_samples += category_labels_batch.size(0)\n",
    "        for k in top_k_category:\n",
    "            corrects = top_k_preds_category[:, :k] == category_labels_batch.unsqueeze(1)\n",
    "            total_category_correct[k] += corrects.any(dim=1).sum().item()\n",
    "\n",
    "        attribute_logits = model.predict_attributes(pixel_values)\n",
    "        attribute_probs = torch.sigmoid(attribute_logits)\n",
    "\n",
    "        all_attribute_preds_probs.append(attribute_probs.cpu())\n",
    "        all_attribute_labels.append(attribute_labels_batch.cpu())\n",
    "\n",
    "    category_accuracies = {f\"Top-{k} Acc (Category)\": total_category_correct[k] / total_category_samples if total_category_samples > 0 else 0\n",
    "                           for k in top_k_category}\n",
    "\n",
    "    attribute_metrics = {}\n",
    "    if all_attribute_labels and all_attribute_preds_probs:\n",
    "        all_attribute_labels_cat = torch.cat(all_attribute_labels, dim=0)\n",
    "        all_attribute_preds_probs_cat = torch.cat(all_attribute_preds_probs, dim=0)\n",
    "\n",
    "        k_attr_values = [1, 3, 5]\n",
    "        for k_attr in k_attr_values:\n",
    "            recalls_at_k_attr = []\n",
    "            for i in range(all_attribute_preds_probs_cat.shape[0]):\n",
    "                sample_probs = all_attribute_preds_probs_cat[i]\n",
    "                sample_labels = all_attribute_labels_cat[i]\n",
    "\n",
    "                num_positive_labels = sample_labels.sum().item()\n",
    "                if num_positive_labels == 0:\n",
    "                    continue\n",
    "\n",
    "                _, top_k_attr_indices = torch.topk(sample_probs, k=k_attr)\n",
    "\n",
    "                retrieved_labels_in_top_k = sample_labels[top_k_attr_indices]\n",
    "\n",
    "                true_positives_in_top_k = retrieved_labels_in_top_k.sum().item()\n",
    "\n",
    "                recall_for_sample = true_positives_in_top_k / num_positive_labels if num_positive_labels > 0 else 0.0\n",
    "                recalls_at_k_attr.append(recall_for_sample)\n",
    "\n",
    "            if recalls_at_k_attr:\n",
    "                 attribute_metrics[f\"Avg Recall@{k_attr} (Attributes)\"] = np.mean(recalls_at_k_attr)\n",
    "            else:\n",
    "                 attribute_metrics[f\"Avg Recall@{k_attr} (Attributes)\"] = 0.0\n",
    "\n",
    "    return category_accuracies, attribute_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_CATEGORIES_IN_DATASET = 50\n",
    "NUM_ATTRIBUTES_IN_DATASET = 463\n",
    "\n",
    "\n",
    "\n",
    "if 'test_dataloader' in locals() and test_dataloader is not None and \\\n",
    "   'model' in locals() and model is not None and \\\n",
    "   hasattr(model, 'predict_category') and hasattr(model, 'predict_attributes'):\n",
    "\n",
    "    print(\"\\nKategori ve Öznitelik Tahmini için Son Değerlendirme:\")\n",
    "    category_scores, attribute_scores = evaluate_classification_tasks(\n",
    "        model,\n",
    "        test_dataloader,\n",
    "        device,\n",
    "        num_categories=NUM_CATEGORIES_IN_DATASET,\n",
    "        num_attributes=NUM_ATTRIBUTES_IN_DATASET,\n",
    "        top_k_category=[1, 3, 5]\n",
    "    )\n",
    "\n",
    "    if category_scores and attribute_scores:\n",
    "        print(\"\\n--- Kategori Tahmini Sonuçları (Liu et al. ile Karşılaştırma İçin) ---\")\n",
    "        for metric, score in category_scores.items():\n",
    "            print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "        print(\"\\n--- Öznitelik Tahmini Sonuçları (Liu et al. ile Karşılaştırma İçin) ---\")\n",
    "        for metric, score in attribute_scores.items():\n",
    "            print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "        print(\"\\nLiu et al. (CVPR 2016) Makalesi Tablo 2 (Category) ve Tablo 3 (Attribute) ile karşılaştırın.\")\n",
    "        print(\"Not: Veri seti bölünmeleri, öznitelik tanımları ve eğitim prosedürleri farklılık gösterebilir.\")\n",
    "    else:\n",
    "        print(\"Kategori/Öznitelik değerlendirmesi yapılamadı.\")\n",
    "elif 'model' in locals() and model is not None and (not hasattr(model, 'predict_category') or not hasattr(model, 'predict_attributes')):\n",
    "    print(\"UYARI: 'model' nesnesinde kategori/öznitelik tahmin metotları ('predict_category', 'predict_attributes') bulunmuyor.\")\n",
    "    print(\"Lütfen modelinizi bu görevler için güncelleyin ve eğitin.\")\n",
    "else:\n",
    "    print(\"Kategori/Öznitelik değerlendirmesi için test_dataloader veya model bulunamadı/uygun değil.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
