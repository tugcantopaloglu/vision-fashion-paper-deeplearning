# VisionFashion: Multiâ€‘Modal Style Embedding

**Multiâ€‘Modal Learning with Vision Transformers and BERT for Fashion Image Analysis and Recommendation**

**Huggingface ðŸ¤—: https://huggingface.co/tugcantopaloglu/visionfashion**

---

## Repository Contents

| Path                  | Description                                                                                                                      |
| --------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| `VisionFashion.ipynb` | Clean Jupyter notebook that trains the ViTâ€¯+â€¯BERT model in two phases (contrastive preâ€‘training and classification fineâ€‘tuning). |
| `VisionFashion.pdf`   | Full research paper describing the methodology, experiments and results.                                                         |
| `data/`               | (Not included) Place the **DeepFashionâ€‘MultiModal** dataset splits here.                                                         |
| `checkpoints/`        | (Optional) Put or save trained model weights here.                                                                               |
| `requirements.txt`    | Minimal Python package list to reproduce the study.                                                                              |

> **Tip:** If you cloned this repo with `git lfs`, the PDF will download automatically.  
> Otherwise, grab it from the link below.

---

## Quick Start

```bash
# 1. Clone the repo and enter it
git clone https://github.com/<yourâ€‘user>/VisionFashion.git
cd VisionFashion

# 2. Create environment (PythonÂ 3.10 recommended)
python -m venv venv
source venv/bin/activate         # on Windows use venv\Scripts\activate
pip install -r requirements.txt

# 3. Download & unzip the DeepFashionâ€‘MultiModal dataset
#    (about 11â€¯GB) into ./data
#    Expected structure:
#    data/
#      train/
#        000123.jpg
#        ...
#      valid/
#      test/
#      captions.csv

# 4. (Optional) Add a pretrained ViT + BERT checkpoint to ./checkpoints

# 5. Launch the notebook
jupyter notebook VisionFashion.ipynb
```

The notebook is fully annotated with **cellâ€‘byâ€‘cell instructions** for:

1. Loading and caching the dataset.
2. PhaseÂ 1 â€“ CLIPâ€‘style contrastive learning of image/text embeddings.
3. PhaseÂ 2 â€“ Fineâ€‘tuning classification heads for _Category_ and _Attribute_ prediction.
4. Evaluation and visualisation of retrieval metrics and learning curves.

---

## Key Results (Reproduced)

| Task                       | Metric     | Score           |
| -------------------------- | ---------- | --------------- |
| **ImageÂ â†”Â Text Retrieval** | R@10 (Iâ†’T) | **0.549â€¯Â±â€¯.01** |
|                            | R@10 (Tâ†’I) | **0.554â€¯Â±â€¯.01** |
| **Category Prediction**    | Topâ€‘1 Acc. | **0.947**       |
| **Attribute Prediction**   | Avg. R@5   | **0.729**       |

These figures match those reported in the accompanying paper. îˆ€fileciteîˆ‚turn0file0îˆ

---

## Paper

The complete methodology, ablation studies and references are provided in **VisionFashion.pdf** (see `VisionFashion.pdf` in this repo).

If you use this codebase, please cite:

```bibtex
@unpublished{topaloglu2025visionfashion,
  author  = {TuÄŸcan TopaloÄŸlu},
  title   = {{VisionFashion}: Multi-Modal Style Embedding Learning with Vision Transformers and BERT for Fashion Image Analysis and Recommendation},
  year    = {2025},
  note    = {Work in progress},
  url     = {https://github.com/tugcantopaloglu/vision-fashion-paper-deeplearning}
}
```

---

## Requirements

```
torch>=2.3
torchvision>=0.18
transformers>=4.41
timm>=0.9
pandas
scikit-learn
matplotlib
```

All dependencies are listed in `requirements.txt`.

---

## Reâ€‘training Tips

- Use a GPU with **at least 16â€¯GB** of VRAM. The full experiment was run on an **A100** (80â€¯GB) but can be reproduced on a **T4/RTXâ€¯4000** with batchÂ 32.
- Lower `batch_size` in the notebook if you encounter OOM.
- Monitor **validation contrastive loss**; best checkpoints typically occur between epochsâ€¯15â€‘20.
- The notebook automatically saves the **best contrastive** and **best classification** checkpoints.

---

## Licence

The code is released under the MIT licence.  
The DeepFashion dataset is subject to its own licence termsâ€”please make sure you comply with them before redistribution.

Happy experimenting! âœ¨
